{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import inspect\n",
    "import shutil\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import importlib\n",
    "from typing import Any, List, Optional, Tuple, Union, Dict\n",
    "import pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, uniform, kstest, entropy, pearsonr, spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.neighbors import NearestNeighbors as KNNsklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats.stats import energy_distance\n",
    "from scipy.spatial.distance import mahalanobis, cdist\n",
    "import uncertainty_toolbox as uct  # https://github.com/uncertainty-toolbox/uncertainty-toolbox\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "\n",
    "matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "matplotlib.rcParams[\"font.family\"] = \"STIXGeneral\"\n",
    "matplotlib.rcParams[\"axes.grid\"] = True\n",
    "matplotlib.rcParams[\"font.size\"] = 18\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "# plot-settings:\n",
    "ps = {\n",
    "    \"GP\": {\"c\": \"red\", \"m\": \"x\"},\n",
    "    \"RF\": {\"c\": \"blue\", \"m\": \"4\"},\n",
    "    \"BNN\": {\"c\": \"orange\", \"m\": \"v\"},\n",
    "    \"DS\": {\"c\": \"black\", \"m\": \"*\"},\n",
    "    \"DE\": {\"c\": \"mediumorchid\", \"m\": \"2\"},\n",
    "    \"RS\": {\"c\": \"palegreen\", \"m\": \"h\"},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Parameters:\n",
    "    seed: int = 0  # random seed\n",
    "    dataset_name: str = \"make_blobs\"  # number of input dimensions\n",
    "    data_dim: int = 2  # number of input dimensions\n",
    "    latent_dim: int = 2  # number of latent space dimensions\n",
    "    n_train: int = 100  # number of training samples\n",
    "    n_test: int = 100  # number of test samples\n",
    "    n_iterations: int = 100  # number of training iterations\n",
    "    gplvm_learning_rate: float = 0.001  # hyperparameter learning rate\n",
    "    plot_it: bool = False  # whether to plot during BO loop\n",
    "    save_it: bool = True  # whether to save progress\n",
    "    gp_latent_init_pca: bool = True  # whether to initialize latent space with PCA solution\n",
    "    savepth: str = os.getcwd() + \"/results/\"\n",
    "    experiment: str = \"\"  # folder name\n",
    "\n",
    "    def __init__(self, kwargs: Dict = {}, mkdir: bool = False) -> None:\n",
    "        self.update(kwargs)\n",
    "        if mkdir and not os.path.isdir(self.savepth):\n",
    "            os.mkdir(self.savepth)\n",
    "        folder_name = f\"z_dim({self.latent_dim})_seed-{self.seed}\"\n",
    "        setattr(\n",
    "            self, \"experiment\", folder_name,\n",
    "        )\n",
    "        setattr(self, \"savepth\", self.savepth + self.experiment + \"/\")\n",
    "        if mkdir and not os.path.isdir(self.savepth):\n",
    "            os.mkdir(self.savepth)\n",
    "            self.save()\n",
    "\n",
    "    def update(self, kwargs, save=False) -> None:\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                raise ValueError(f\"Parameter {key} not found\")\n",
    "        if save:\n",
    "            self.save()\n",
    "\n",
    "    def save(self) -> None:\n",
    "        json_dump = json.dumps(asdict(self))\n",
    "        with open(self.savepth + \"parameters.json\", \"w\") as f:\n",
    "            f.write(json_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, parameters: Parameters) -> None:\n",
    "        self.__dict__.update(asdict(parameters))\n",
    "        self.generate()\n",
    "\n",
    "    def generate(self):\n",
    "        if self.dataset_name == \"make_blobs\":\n",
    "            self.X, self.y = make_blobs(\n",
    "                n_samples=self.n_train,\n",
    "                n_features=self.data_dim,\n",
    "                centers=2,\n",
    "                cluster_std=1 / (10 * self.data_dim),\n",
    "            )\n",
    "            self.X = self.X.transpose()\n",
    "        else:\n",
    "            # Load the MNIST data set and isolate a subset of it.\n",
    "            (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "            self.X = x_train[: self.n_train, ...].astype(np.float64) / 256.0\n",
    "            self.y = y_train[: self.n_train]\n",
    "            self.X = self.X.reshape(self.n_train, -1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPLVM(object):\n",
    "    def __init__(self, parameters: Parameters) -> None:\n",
    "        self.__dict__.update(asdict(parameters))\n",
    "        np.random.seed(self.seed)\n",
    "        self.trainable_variables = []\n",
    "        self.init_hyperparameters()\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=self.gplvm_learning_rate)\n",
    "        self.summary = {}\n",
    "\n",
    "    def fit(self, dataset: Dataset):\n",
    "        \"\"\"X must be shape [D,N] \"\"\"\n",
    "        self.X = dataset.X\n",
    "        self.y = dataset.y\n",
    "        self.init_z()\n",
    "        self.train()\n",
    "\n",
    "    def init_hyperparameters(self):\n",
    "        self.unconstrained_amplitude = tf.Variable(np.float64(1.0), name=\"amplitude\")\n",
    "        self.unconstrained_length_scale = tf.Variable(\n",
    "            np.float64(1.0), name=\"length_scale\"\n",
    "        )\n",
    "        self.unconstrained_observation_noise = tf.Variable(\n",
    "            np.float64(1.0), name=\"observation_noise\"\n",
    "        )\n",
    "        self.trainable_variables.append(self.unconstrained_amplitude)\n",
    "        self.trainable_variables.append(self.unconstrained_length_scale)\n",
    "        self.trainable_variables.append(self.unconstrained_observation_noise)\n",
    "\n",
    "    def init_z(self):\n",
    "        if self.gp_latent_init_pca:\n",
    "            self.z_init = PCA(self.latent_dim).fit_transform(self.X.transpose())\n",
    "        else:\n",
    "            self.z_init = np.random.normal(size=(self.n_train, self.latent_dim))\n",
    "\n",
    "        self.latent_index_points = tf.Variable(self.z_init, name=\"latent_index_points\")\n",
    "        self.trainable_variables.append(self.latent_index_points)\n",
    "\n",
    "    def create_kernel(self) -> tf.Tensor:\n",
    "        amplitude = tf.math.softplus(1e-8 + self.unconstrained_amplitude)\n",
    "        length_scale = tf.math.softplus(1e-8 + self.unconstrained_length_scale)\n",
    "        kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)\n",
    "        return kernel\n",
    "\n",
    "    def loss_fn(self) -> tf.Tensor:\n",
    "        observation_noise_variance = tf.math.softplus(\n",
    "            1e-8 + self.unconstrained_observation_noise\n",
    "        )\n",
    "        gp = tfd.GaussianProcess(\n",
    "            kernel=self.create_kernel(),\n",
    "            index_points=self.latent_index_points,\n",
    "            observation_noise_variance=observation_noise_variance,\n",
    "        )\n",
    "        log_probs = gp.log_prob(self.X, name=\"log_prob\")\n",
    "        return -tf.reduce_mean(log_probs)\n",
    "\n",
    "    @tf.function(autograph=False, jit_compile=True)\n",
    "    def train_step(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.loss_fn()\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss_value\n",
    "\n",
    "    def train(self):\n",
    "        lips = np.zeros((self.n_iterations, self.data_dim, self.latent_dim), np.float64)\n",
    "        self.loss_history = np.zeros((self.n_iterations,), np.float64)\n",
    "        for i in range(self.n_iterations):\n",
    "            loss = self.train_step()\n",
    "            lips[i] = self.latent_index_points.numpy()\n",
    "            self.loss_history[i] = loss.numpy()\n",
    "\n",
    "        self.z_final = lips[[np.argmin(self.loss_history)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self, parameters: Parameters) -> None:\n",
    "        self.__dict__.update(asdict(parameters))\n",
    "        self.dataset = Dataset(parameters)\n",
    "        self.model = GPLVM(parameters)\n",
    "\n",
    "    def run(self):\n",
    "        self.model.fit(self.dataset)\n",
    "        self.plot_learning_curve()\n",
    "\n",
    "    def plot_learning_curve(self):\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.plot(self.model.loss_history)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100,2) into shape (2,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-530bd4f44a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-b976ad20bb32>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e56998ea8c00>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e56998ea8c00>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mlips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_index_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100,2) into shape (2,2)"
     ]
    }
   ],
   "source": [
    "parameters = Parameters()\n",
    "experiment = Experiment(parameters)\n",
    "experiment.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec63b00df97640358dc9c6b72bcb00746af5e466aec71cbdbcdc20ab3c51910b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
