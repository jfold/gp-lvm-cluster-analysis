{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import inspect\n",
    "import shutil\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import importlib\n",
    "from typing import Any, List, Optional, Tuple, Union, Dict\n",
    "import pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, uniform, kstest, entropy, pearsonr, spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "from sklearn.neighbors import NearestNeighbors as KNNsklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats.stats import energy_distance\n",
    "from scipy.spatial.distance import mahalanobis, cdist\n",
    "import uncertainty_toolbox as uct  # https://github.com/uncertainty-toolbox/uncertainty-toolbox\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "\n",
    "matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "matplotlib.rcParams[\"font.family\"] = \"STIXGeneral\"\n",
    "matplotlib.rcParams[\"axes.grid\"] = True\n",
    "matplotlib.rcParams[\"font.size\"] = 18\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "# plot-settings:\n",
    "ps = {\n",
    "    \"GP\": {\"c\": \"red\", \"m\": \"x\"},\n",
    "    \"RF\": {\"c\": \"blue\", \"m\": \"4\"},\n",
    "    \"BNN\": {\"c\": \"orange\", \"m\": \"v\"},\n",
    "    \"DS\": {\"c\": \"black\", \"m\": \"*\"},\n",
    "    \"DE\": {\"c\": \"mediumorchid\", \"m\": \"2\"},\n",
    "    \"RS\": {\"c\": \"palegreen\", \"m\": \"h\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Parameters:\n",
    "    seed: int = 0  # random seed\n",
    "    dataset_name: str = \"make_blobs\"  # number of input dimensions\n",
    "    data_dim: int = 2  # number of input dimensions\n",
    "    latent_dim: int = 2  # number of latent space dimensions\n",
    "    n_train: int = 100  # number of training samples\n",
    "    n_test: int = 100  # number of test samples\n",
    "    n_iterations: int = 1000  # number of training iterations\n",
    "    gplvm_learning_rate: float = 0.001  # hyperparameter learning rate\n",
    "    cluster_std: float = None \n",
    "    plot_it: bool = False  # whether to plot during BO loop\n",
    "    save_it: bool = True  # whether to save progress\n",
    "    gp_latent_init_pca: bool = False  # whether to initialize latent space with PCA solution\n",
    "    savepth: str = os.getcwd() + \"/results/\"\n",
    "    experiment: str = \"\"  # folder name\n",
    "\n",
    "    def __init__(self, kwargs: Dict = {}, mkdir: bool = False) -> None:\n",
    "        self.update(kwargs)\n",
    "        if mkdir and not os.path.isdir(self.savepth):\n",
    "            os.mkdir(self.savepth)\n",
    "        folder_name = f\"z_dim({self.latent_dim})_seed-{self.seed}\"\n",
    "        setattr(\n",
    "            self, \"experiment\", folder_name,\n",
    "        )\n",
    "        setattr(self, \"savepth\", self.savepth + self.experiment + \"/\")\n",
    "        if mkdir and not os.path.isdir(self.savepth):\n",
    "            os.mkdir(self.savepth)\n",
    "            self.save()\n",
    "\n",
    "    def update(self, kwargs, save=False) -> None:\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                raise ValueError(f\"Parameter {key} not found\")\n",
    "        if save:\n",
    "            self.save()\n",
    "\n",
    "    def save(self) -> None:\n",
    "        json_dump = json.dumps(asdict(self))\n",
    "        with open(self.savepth + \"parameters.json\", \"w\") as f:\n",
    "            f.write(json_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, parameters: Parameters) -> None:\n",
    "        self.__dict__.update(asdict(parameters))\n",
    "        np.random.seed(self.seed)\n",
    "        self.generate()\n",
    "\n",
    "    def generate(self):\n",
    "        if self.dataset_name == \"make_blobs\":\n",
    "            if self.cluster_std is None:\n",
    "                self.cluster_std = 1 / (10*self.data_dim)\n",
    "            self.X, self.y = make_blobs(\n",
    "                n_samples=self.n_train,\n",
    "                n_features=self.data_dim,\n",
    "                centers=2,\n",
    "                cluster_std=self.cluster_std,\n",
    "            )\n",
    "            self.X = self.X.transpose()\n",
    "        else:\n",
    "            # Load the MNIST data set and isolate a subset of it.\n",
    "            (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "            self.X = x_train[: self.n_train, ...].astype(np.float64) / 256.0\n",
    "            self.y = y_train[: self.n_train]\n",
    "            self.X = self.X.reshape(self.n_train, -1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPLVM(object):\n",
    "    def __init__(self, parameters: Parameters) -> None:\n",
    "        self.__dict__.update(asdict(parameters))\n",
    "        np.random.seed(self.seed)\n",
    "        self.trainable_variables = []\n",
    "        self.init_hyperparameters()\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=self.gplvm_learning_rate)\n",
    "        self.summary = {}\n",
    "\n",
    "    def fit(self, dataset: Dataset):\n",
    "        \"\"\"X must be shape [D,N] \"\"\"\n",
    "        self.X = dataset.X\n",
    "        self.y = dataset.y\n",
    "        self.init_z()\n",
    "        self.train()\n",
    "\n",
    "    def init_hyperparameters(self):\n",
    "        self.unconstrained_amplitude = tf.Variable(np.float64(1.0), name=\"amplitude\")\n",
    "        self.unconstrained_length_scale = tf.Variable(\n",
    "            np.float64(1.0), name=\"length_scale\"\n",
    "        )\n",
    "        self.unconstrained_observation_noise = tf.Variable(\n",
    "            np.float64(1.0), name=\"observation_noise\"\n",
    "        )\n",
    "        self.trainable_variables.append(self.unconstrained_amplitude)\n",
    "        self.trainable_variables.append(self.unconstrained_length_scale)\n",
    "        self.trainable_variables.append(self.unconstrained_observation_noise)\n",
    "\n",
    "    def init_z(self):\n",
    "        if self.gp_latent_init_pca:\n",
    "            self.z_init = PCA(self.latent_dim).fit_transform(self.X.transpose())\n",
    "        else:\n",
    "            self.z_init = np.random.normal(size=(self.n_train, self.latent_dim))\n",
    "\n",
    "        self.latent_index_points = tf.Variable(self.z_init, name=\"latent_index_points\")\n",
    "        self.trainable_variables.append(self.latent_index_points)\n",
    "\n",
    "    def create_kernel(self) -> tf.Tensor:\n",
    "        amplitude = tf.math.softplus(1e-8 + self.unconstrained_amplitude)\n",
    "        length_scale = tf.math.softplus(1e-8 + self.unconstrained_length_scale)\n",
    "        kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)\n",
    "        return kernel\n",
    "\n",
    "    def loss_fn(self) -> tf.Tensor:\n",
    "        observation_noise_variance = tf.math.softplus(\n",
    "            1e-8 + self.unconstrained_observation_noise\n",
    "        )\n",
    "        gp = tfd.GaussianProcess(\n",
    "            kernel=self.create_kernel(),\n",
    "            index_points=self.latent_index_points,\n",
    "            observation_noise_variance=observation_noise_variance,\n",
    "        )\n",
    "        log_probs = gp.log_prob(self.X, name=\"log_prob\")\n",
    "        return -tf.reduce_mean(log_probs)\n",
    "\n",
    "    @tf.function(autograph=False, jit_compile=True)\n",
    "    def train_step(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.loss_fn()\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss_value\n",
    "\n",
    "    def train(self):\n",
    "        lips = np.zeros((self.n_iterations, self.n_train, self.latent_dim), np.float64)\n",
    "        self.loss_history = np.zeros((self.n_iterations,), np.float64)\n",
    "        for i in range(self.n_iterations):\n",
    "            loss = self.train_step()\n",
    "            lips[i] = self.latent_index_points.numpy()\n",
    "            self.loss_history[i] = loss.numpy()\n",
    "\n",
    "        self.z_final = lips[[np.argmin(self.loss_history)]].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results(object):\n",
    "\n",
    "    def plot_learning_curve(self):\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.plot(self.model.loss_history)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_latent_space(self, cluster_centroids:np.ndarray = None):\n",
    "        # Plot the latent locations before and after training\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.title(\"Before training\")\n",
    "        plt.grid(False)\n",
    "        plt.scatter(x=self.model.z_init[:, 0], y=self.model.z_init[:, 1],\n",
    "                c=self.model.y, cmap=plt.get_cmap('Paired'), s=50,alpha=0.4)\n",
    "        plt.xlabel(r\"$z_1$\")\n",
    "        plt.ylabel(r\"$z_2$\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.title(\"After training\")\n",
    "        plt.grid(False)\n",
    "        plt.scatter(x=self.model.z_final[:, 0], y=self.model.z_final[:, 1],\n",
    "                c=self.model.y, cmap=plt.get_cmap('Paired'), s=50,alpha=0.4)\n",
    "        if cluster_centroids is not None:\n",
    "            plt.plot(cluster_centroids[:,0],cluster_centroids[:,1],\"x\",label=\"centroid\")\n",
    "        plt.xlabel(r\"$z_1$\")\n",
    "        plt.ylabel(r\"$z_2$\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(Results):\n",
    "    def __init__(self, parameters: Parameters) -> None:\n",
    "        self.__dict__.update(asdict(parameters))\n",
    "        self.dataset = Dataset(parameters)\n",
    "        self.model = GPLVM(parameters)\n",
    "\n",
    "    def run(self):\n",
    "        self.model.fit(self.dataset)\n",
    "        self.Z = self.model.z_final\n",
    "        self.k_means = KMeans(n_clusters=2,n_init=20).fit(self.Z)\n",
    "        self.y_preds = self.k_means.predict(self.Z)\n",
    "        self.nmi = nmi(self.model.y,self.y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(10):\n",
    "\tparameters = Parameters({\"n_iterations\":30000, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"data_dim\":2, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"n_train\":10, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"seed\":seed}\n",
    "\t)\n",
    "\texperiment = Experiment(parameters)\n",
    "\texperiment.run()\n",
    "\tprint(experiment.nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(experiment.y_preds)\n",
    "print(experiment.dataset.y)\n",
    "experiment.plot_learning_curve()\n",
    "experiment.plot_latent_space(experiment.k_means.cluster_centers_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec63b00df97640358dc9c6b72bcb00746af5e466aec71cbdbcdc20ab3c51910b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
